# Ch 8. DQN Extensions

## 들어가며

딥마인드에서 DQN을 발표한 이래, 많은 발전이 이루어졌다. 특히 2017년 딥마인드에서 발표한 Rainbow는 DQN에서 여러 개선을 해서 Atari 게임에서 좋은 성능을 보였다.

책에서 다룬 DQN Extensions는 총 6가지로, N-step DQN, Double DQN, Noisy networks, Prioritized replay buffer, Dueling DQN, Categorical DQN이다. 이 개선안들을 종합하고 비교해 최종적으로 만들어진 것이 Rainbow인 것이다.

## N-step DQN

### 이론

$Q(s_t,a_t) = r_t + \gamma \  \underset{a}{max} Q(s_{t+1}, a_{t+1})$

$= r_t + \gamma \  \underset{a}{max} [r_{a,t+1} + \gamma \ \underset{a^{'}}{max}Q(s_{t+2}, a^{'})]$

$= r_t +\gamma \ r_{t+1} + \gamma^2 \  \underset{a^{'}}{max} Q(s_{t+2}, a^{'})$

$= r_t +\gamma \ r_{t+1} + \gamma^2 \  r_{t+2} + ... + \gamma^{n-1} r_{t+n-1} + \gamma ^ n \underset{a_n}{max} Q(s_{t+n}, a_n)$

Q learning에서의 update 식은 이런 식으로 n step까지 계속 이어질 수 있다. 가장 처음의 식을 one-step equation, 그 다음 식을 two-step equation, 그리고 n번 이어진 식을 n-step equation이라고 하는데, N-step DQN은 n-step equation이 더 빨리 수렴한다는 점에서 착안한 아이디어이다.

예시로, 아래와 같은 환경을 생각해보자.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__2.28.24.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__2.28.24.png)

이 환경에서는 3가지 업데이트가 존재하는데, one-step equation은 세 번의 iteration에 수렴하지만, two-step equation은 두 번의 iteration 만에 수렴한다. (수렴한 값은 각각 22.6, 22, 15이다.) 자세한 증명은 아래와 같다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/1.jpeg](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/1.jpeg)

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/2.jpeg](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/2.jpeg)

이렇게 n-step equation의 수렴 속도가 빠른 것은, useless한 스텝을 제외할 수 있기 때문이다. Q-learning에서는 처음에 모든 Q value를 어떤 랜덤한 값으로 초기화한다. One-step equation에서는, t번째 state의 Q value를 유효하게 업데이트 하기 위해 t+1번째 state의 Q value의 업데이트가 선행되어야 하고, 이는 재귀적으로 이어진다. 결국 그전까지의 iteration에서는 유효하지 않은, 랜덤한 값에 의한 업데이트가 이루어진다. 반대로 n-step equation은, n이 커질수록 더 빨리 업데이트된 state들을 이용해 상대적으로 유효한 업데이트가 가능한 것이다.

그러면 n이 커질수록, 예를 들어 100, DQN의 수렴 속도가 빨라질까? 책에서는 수렴이 빨라지기는커녕, 수렴이 불가능하다고 말한다. 위와 같은 환경에서는 action에 대한 max operation이 없지만, 실제 환경에서는 max operation이 존재한다. Optimal policy에 도달하기 전 agent의 action에 따라 n-step equation으로 업데이트하면, 리워드를 더 적게 받아, state의 optimal Q value보다 작은 값이 된다. 그러므로 n이 커질수록, optimal한 Q value와의 차이가 커져 더욱 부정확해지는 것이다. 게다가 현재의 policy로 행동한 데이터만을 사용하는 on-policy와 다르게 off-policy인 DQN은 과거의 부정확한 policy의 observation을 사용한다. 그래서 n이 커질수록 부정확한 정도도 커진다.

그러면 n을 어떻게 설정해야 하는 것일까? 책에서는 2,3 정도의 n 값이 적절하다고 말한다. 적당히 작은 값의 n을 고르면, 수렴 속도가 빨라지는 것이다.

### 구현

책의 Basic DQN 코드에서 아래 두 부분이 바뀐다.

먼저 steps_count를 n으로 설정해 one-step이 아니라 n-step equation을 통해 value를 계산한다.

```python
exp_source = ptan.experience.ExperienceSourceFirstLast(
	  env, agent, gamma=params.gamma, steps_count=args.n)
```

```python
for exp in super(ExperienceSourceFirstLast, self).__iter__():
    if exp[-1].done and len(exp) <= self.steps: 
		# episode의 끝이거나 설정한 step보다 exp의 길이가 짧을 경우
        last_state = None
        elems = exp
    else:
		# 그렇지 않은 경우, 마지막 state는 last_state에 저장되고,
		# elems에 마지막 원소를 제외한 exp가 저장된다.
        last_state = exp[-1].state
        elems = exp[:-1]
    total_reward = 0.0
    for e in reversed(elems):
		# total_reward, 즉 Q value 계산 코드
		# n step이면 이 for문이 최대 n번 반복될 것이고, 이는 n-step equation과 같다.
        total_reward *= self.gamma
        total_reward += e.reward
    yield ExperienceFirstLast(state=exp[0].state, action=exp[0].action,
                              reward=total_reward, last_state=last_state) 
		# iteration을 할 때 yield에 저장된 값이 return된다.
```

```python
loss_v = common.calc_loss_dqn(
	  batch, net, tgt_net.target_model,
	  gamma=params.gamma**args.n, device=device)
		# n-step equation의 마지막 항에서는 감마의 n승이 곱해지기 때문이다.
```

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.08.23.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.08.23.png)

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.08.57.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.08.57.png)

## Double DQN

### 이론

Basic DQN은 Q value를 optimal value보다 크게 도출하는 한계점이 있었고, 이 때문에 성능을 저하시키거나 suboptimal policy로 수렴하곤 했다. 수학적인 근거는 책에 밝혀져 있지 않고, Bellman 업데이트에서의 max 계산 때문이라고만 러프하게 나와있다. 그래서 Double DQN은 Bellman 업데이트의 수식만을 살짝 바꾼 형태다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.41.28.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.41.28.png)

기존 DQN은 target network의 Q value를 최대화하는 action을 찾고, 그 값을 취한다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.35.34.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.35.34.png)

반면 Double DQN은 trained network의 Q value를 최대화하는 action을 고르지만, 이에 대한 Q value는 target network의 것을 취한다.

Double DQN은 Basic DQN은 overestimation 현상을 완전히 해결했다.

### 구현

Loss를 계산하는 부분만 수정이 필요하다.

```python
def calc_loss_double_dqn(batch, net, tgt_net, gamma,
		device="cpu", double=True):
```

여기서 double이 True면 Double DQN을 사용한다는 뜻이다.

```python
actions_v = actions_v.unsqueeze(-1)
state_action_vals = net(states_v).gather(1, actions_v)
state_action_vals = state_action_vals.squeeze(-1)
next_states_v = torch.tensor(next_states).to(device)
if double:
		next_state_acts = net(next_states_v).max(1)[1]
		# Trained network를 이용해 max가 되는 action을 고른다.
		next_state_acts = next_state_acts.unsqueeze(-1) 
		# 마지막 차원을 더한다. e.g. (3,2) → (3,2,1)
		next_state_vals = tgt_net(next_states_v).gather(
		1, next_state_acts).squeeze(-1)
		# gather을 통해 원하는 2번째 차원에서 원하는 index만(next_state_acts)만 뽑는다.
else:
		next_state_vals = tgt_net(next_states_v).max(1)[0]
next_state_vals[done_mask] = 0.0
exp_sa_vals = next_state_vals.detach()*gamma+rewards_v 
# 모델이 예측한 Q value를 계산한다.
```

Bellman equation의 수정은 위와 같이 간단하게 이뤄질 수 있다.

```python
def calc_values_of_states(states, net, device="cpu"):
		 mean_vals = []
		 for batch in np.array_split(states, 64):
		     states_v = torch.tensor(batch).to(device)
		     action_values_v = net(states_v)
		     best_action_values_v = action_values_v.max(1)[0]
		     mean_vals.append(best_action_values_v.mean().item())
		 return np.mean(mean_vals)
```

마지막으로 overestimation 문제를 확인하기 위해, held-out states (딥러닝에서의 unseen dataset, 즉 test 데이터셋) 에서 최대가 되는 Q value의 평균을 구해 저장하는 함수를 새로 작성해야한다.

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.57.15.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__10.57.15.png)

Reward dynamics for double and baseline DQN

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.01.14.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.01.14.png)

Values predicted by the network for held-out states

Reward dynamics는 Double DQN에서 더 좋았으나, 게임을 클리어하는데 걸린 시간은 비슷했다. 그러나 Q value의 overestimation 문제는 확실히 개선된 것을 볼 수 있다.

## Noisy networks

### 이론

기존의 DQN은 exploration versus exploitaion dilemma를 위해 epsilon-greedy method을 이용했다. stationarity가 높은, 즉, 확률분포가 시간에 대한 영향을 적게 받는, 작은 환경과 적은 episode에서는 좋은 효과를 보였지만, 외에는 그렇지 않았다.

Noisy network는 FCN의 weight에 노이즈 값을 더해 이 문제를 해결한 모델이다. 노이즈를 더하는 방법은 크게 두 가지가 존재한다.

1. Independent Gaussian noise: FCN의 모든 레이어에 대해 정해진 normal distribution에서 추출한 random value를 더해준다. 이 때, normal distribution의 평균과 표준편차는 각 레이어마다 학습해서 설정한다.
2. Factorized Gaussian noise: 랜덤하게 추출되는 값의 수를 줄이기 위해, 모든 레이어에 대해 표준편차와 평균을 학습해서 그에 따른 분포에서 값을 추출하는 것이 아니라, 두 개의 벡터만 랜덤하게 추출한다. 하나는 input size이고, 하나는 output size여서, 둘을 외적하면 FCN의 레이어에 대한 random matrix를 계산할 수 있다.

### 구현

**Independent Gaussian noise**

```python
class NoisyLinear(nn.Linear):
def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):
		super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)
		w = torch.full((out_features, in_features), sigma_init)
		self.sigma_weight = nn.Parameter(w)
		# 표준편차를 저장하기 위한 matrix w를 만들고, 학습하기 위해 nn.Parameter()로 캐스팅해준다.
		z = torch.zeros(out_features, in_features)
		self.register_buffer("epsilon_weight", z)
		# 레이어의 bias를 위한 matrix z를 만든다.
		if bias:
		# bias를 준다면
				w = torch.full((out_features,), sigma_init)
				# sigma_init 값으로 채운다.
				self.sigma_bias = nn.Parameter(w)
				z = torch.zeros(out_features)
				self.register_buffer("epsilon_bias", z)
		self.reset_parameters()
```

```python
def reset_parameters(self):
# nn.Linear의 레이어 초기화 메소드를 오버라이딩 
		std = math.sqrt(3 / self.in_features)
		self.weight.data.uniform_(-std, std)
		self.bias.data.uniform_(-std, std)
```

```python
def forward(self, input):
		self.epsilon_weight.normal_()
		bias = self.bias
		if bias is not None:
				self.epsilon_bias.normal_()
				bias = bias + self.sigma_bias * self.epsilon_bias.data
		v = self.sigma_weight * self.epsilon_weight.data + self.weight
		# self.weight에 noise를 더해준다.
		return F.linear(input, v, bias)
		# y = xA.T + b 선형변환을 진행한다. 즉, weight을 곱하고 bias를 더해준다.
```

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.36.22.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.36.22.png)

Reward dynamics for noisy net and baseline DQN

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.38.55.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-06-30__11.38.55.png)

Noise level changes during the training

Noisy net의 reward dynamics는 기존의 DQN보다 더 좋았을뿐더러, 18점의 평균 score에 훨씬 빨리 도달했다. 또 신호/노이즈는 모든 레이어에서 빠르게 증가했다. 이는 학습을 거듭할수록 환경을 더 잘배우게 되기 때문이다. 하지만 거의 마지막 step에서는 오히려 감소하는데, 책에서는 이렇게 말한다. "After reaching high score levels, the agent basically knows how to play at a good level, but still needs to "polish" its actions to improve the results even more."

## Prioritized replay buffer

### 이론

이 기법은 replay buffer를 더 효과적으로 샘플링하기 위한 기법으로, training loss를 이용해 샘플의 우선순위를 설정한다.

기존의 DQN은 replay buffer을 이용해 과거의 observation을 사용해 시간에 따른 상관관계를 없애려 했다. 하지만 prioritized replay buffer을 제시한 저자들은 replay buffer에서 데이터를 추출하는 방식에 의문점을 제기했다. Random하게 추출하는 것보다 training loss에 proportional한 확률로 sampling하는 게 더 낫다는 것이다.

각 sample의 priority는 $P(i) = {p_i^{a} \over \sum_k p_k^{\alpha}}$로 계산한다. 이 때 $p_i$의 정의는 다양하지만, loss에 비례하는 것이 가장 보편적이다. 분모는 replay buffer에 존재하는 모든 sample의 priority를 더한 값이고, $\alpha$는 priority의 강도를 결정하는 하이퍼파라미터다. 0이면 모든 확률은 같을 것이고, 클수록 priority의 차이가 커져 priority가 높은 sample이 사용될 확률이 높아진다. (논문에서는 $\alpha$의 시작 값으로 0.6을 제시한다.)

또 이렇게 priority에 따라 추출하면 어떤 sample이 훨씬 자주 학습에 이용되기 때문에, 학습을 돕기 위해 bias를 적용한다. 각 sample의 loss에 곱해지는 sample weight는 $w_i = (N \cdot P(i))^{-\beta}$로 계산한다. $\beta$는 0과 1 사이의 값으로, 학습이 진행됨에 따라 서서히 1까지 증가시켜 사용한다.

### 구현

```python
BETA_START = 0.4
BETA_FRAMES = 100000

class PrioReplayBuffer:
		def __init__(self, exp_source, buf_size, prob_alpha=0.6):
				...
				self.prob_alpha = prob_alpha
				self.priorities = np.zeros((buf_size, ), dtype=np.float32)
				self.beta = BETA_START
				# priorities와 alpha, beta를 저장하는 변수가 추가로 필요하다.

		def update_beta(self, idx):
				# update beta 0.4 ... -> ... -> 1
				v = BETA_START + idx * (1.0 - BETA_START) / BETA_FRAMES
				self.beta = min(1.0, v)
				return self.beta

		def populate(self, count):
				max_prio = self.priorities.max() if self.buffer else 1.0
				for _ in range(count):
						sample = next(self.exp_source_iter)
						# sample에 대한 iteration
						if len(self.buffer) < self.capacity:
								self.buffer.append(sample)
								# buffer가 capacity에 도달하지 않았다면, 단순히 추가해준다.
						else:
								self.buffer[self.pos] = sample
								# buffer가 capacity에 도달했다면, 가장 오래된 sample에 overwrite한다.
						self.priorities[self.pos] = max_prio
						self.pos = (self.pos + 1) % self.capacity 
		
		def sample(self, batch_size):
				if len(self.buffer) == self.capacity:
						prios = self.priorities
				else:
						prios = self.priorities[:self.pos]
				probs = prios ** self.prob_alpha
				probs /= probs.sum()
				# alpha를 이용해 probs를 계산한다
				indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[idx] for idx in indices]
				# probs에 비례하는 확률로 sample를 선택한다
				total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
	      weights /= weights.max()
				# beta를 이용해 weight에 bias를 적용한다.
        return samples, indices, np.array(weights, dtype=np.float32)
		
		def update_priorities(self, batch_indices, batch_priorities):
		    for idx, prio in zip(batch_indices,batch_priorities):
		        self.priorities[idx] = prio
				# update new priorities
```

또한 weight를 적용할 수 있는 새로운 MSE loss 함수가 필요하다.

```python
def calc_loss(batch, batch_weights, net, tgt_net, gamma, device="cpu"):
		states, actions, rewards, dones, next_states = common.unpack_batch(batch)
		states_v = torch.tensor(states).to(device)
		actions_v = torch.tensor(actions).to(device)
		rewards_v = torch.tensor(rewards).to(device)
		done_mask = torch.BoolTensor(dones).to(device)
		batch_weights_v = torch.tensor(batch_weights).to(device)
		actions_v = actions_v.unsqueeze(-1)
		state_action_vals = net(states_v).gather(1, actions_v)
		state_action_vals = state_action_vals.squeeze(-1)
		with torch.no_grad():
				next_states_v = torch.tensor(next_states).to(device)
				next_s_vals = tgt_net(next_states_v).max(1)[0]
				next_s_vals[done_mask] = 0.0
				exp_sa_vals = next_s_vals.detach() * gamma + rewards_v
		l = (state_action_vals - exp_sa_vals) ** 2
		losses_v = batch_weights_v * l
		return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()
```

마지막으로, batch data에 sample한 replay 데이터, 그리고 sample의 weight으로 구성된 새로운 batch를 process 하는 함수가 필요하다.

```python
def process_batch(engine, batch_data):
		batch, batch_indices, batch_weights = batch_data
		optimizer.zero_grad()
		loss_v, sample_prios = calc_loss(
				batch, batch_weights, net, tgt_net.target_model,
				gamma=params.gamma, device=device)
		loss_v.backward()
		optimizer.step()
		buffer.update_priorities(batch_indices, sample_prios)
		epsilon_tracker.frame(engine.state.iteration)
		if engine.state.iteration % params.target_net_sync == 0:
				tgt_net.sync()
		return {
				"loss": loss_v.item(),
				"epsilon": selector.epsilon,
				"beta": buffer.update_beta(engine.state.iteration),
		}
```

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.28.35.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.28.35.png)

Prioritized replay buffer (right) in comparison to basic DQN (left)

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.26.18.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.26.18.png)

The comparison of loss during the training

Prioritized replay buffer을 적용하면, 시간에 따른 reward dynamics는 기존의 DQN과 비슷했지만 연산량에 의한 것이고, episode에 따른 reward dynamics는 기존의 DQN보다 좋았다. 또, loss도 잘 감소했다.

## Dueling DQN

### 이론

Dueling DQN의 저자는 Q value를 state의 value에 대한 항 V(s)와 현재 state에서 action에 대한 효과에 대한 항 A(s,a)로 나누었다.

$Q(s,a) = V(s) + A(s,a)$

V(s)는 value iteration에서 사용된, 단순히 discounted된 미래 보상의 합이다. A(s,a)는 Q(s,a)와 V(s)의 차이를 극복하기 위한 항으로, 책에서는 "how much extra reward some particular action from the state brings us"라고 말하고 있다. Action은 더 많은 reward를 얻을 수 있는 방향으로 agent를 이끌 수도 있고, 그렇지 않을 수도 있기 때문에 A(s,a)는 음수와 양수 모두 가진다.

Dueling DQN에서는 V(s)와 A(s,a)를 분리함으로서 학습의 안정성을 높이고, 수렴을 가속하고, Atari 환경에서 좋은 결과를 냈다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.34.59.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.34.59.png)

A basic DQN (top) and dueling architecture (bottom)

DQN은 각 action에 대한 Q value가 output이 되지만, dueling DQN에서는 마지막 convolution feature가 두 가지 경로로 나누어진다. 하나는 V(s)를 예측하고, 다른 하나는 A(s,a)를 예측하는데, V(s)는 모든 action에서 똑같은 하나의 값이기 때문에 예측한 A(s,a)에 모두 더해준다.

각각의 경로가 우리가 원하는대로 하나는 V(s)를, 다른 하나는 A(s,a)를 예측할 수 있도록 도출된 A(s)의 합은 0으로 만들어야 한다. 즉, V(s)=0, A(s)=[1,2,3,4]가 아니라, V(s)=2.5, A(s)=[-1.5,-0.5,0.5,1.5]가 되어야 한다. 저자는 A(s)의 합을 0으로 만들기 위해 Q(s,a)의 Bellman update function을 아래와 같이 수정했다.

$Q(s,a) = V(s)+A(s,a)- {1 \over N} \sum_k A(s,k)$  

### 구현

```python
class DuelingDQN(nn.Module):
		def __init__(self, input_shape, n_actions):
				super(DuelingDQN, self).__init__()
				self.conv = ... # 기존의 DQN과 convolution layer는 같다
				conv_out_size = self._get_conv_out(input_shape)
				self.fc_adv = nn.Sequential(
						nn.Linear(conv_out_size, 256),
						nn.ReLU(),
						nn.Linear(256, n_actions)
				)
				self.fc_val = nn.Sequential(
						nn.Linear(conv_out_size, 256),
						nn.ReLU(),
						nn.Linear(256, 1)
				)
				# A(s,a)를 예측하는 fc_adv FCN 레이어와 V(s)를 예측하는 fc_val FCN 레이어를 정의
		
		def adv_val(self, x):
				fx = x.float() / 256
				conv_out = self.conv(fx).view(fx.size()[0], -1)
				return self.fc_adv(conv_out), self.fc_val(conv_out)
		
		def forward(self, x):
				adv, val = self.adv_val(x)
				return val + (adv - adv.mean(dim=1, keepdim=True))
				# 이론대로 Q(s,a)를 계산
```

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.45.38.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.45.38.png)

The reward dynamic of dueling DQN compared to the baseline version

Dueling DQN의 reward dynamics는 기존의 DQN보다 높았다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.46.42.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.46.42.png)

The advantage (left) and value (right) on a fixed set of states

A(s,a)는 거의 0에 가까운 값이었지만, 시간이 지날수록 더 발전되었다.

## Categorical DQN

### 이론

복잡한 환경에서는 Q value라는 하나의 값이 future reward와 가치를 표현하기 부족하다는 점에서 착안한 것이 future reward를 stochastic하게 표현하는 categorical DQN이다.

**Commuter Scenario**

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__1.00.20.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__1.00.20.png)

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__1.00.36.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__1.00.36.png)

통근 방법을 정할 때, 차가 평균 소요 시간이 기차보다 작기 때문에 차를 이용한 통근이 좋아 보인다. 하지만 전체 분포를 보면, 차는 variance가 높기 때문에, 확실하게 60분 안에 도착하고 싶으면 기차를 타는 것이 낫다. 이런 예시처럼 단 하나의 값으로 표현되는 Q value는 많은 경우에서 실제 분포를 표현하기 어렵다.

Categorical DQN은 모든 action에 대해, Q value의 분포를 구한다. Categorical DQN의 Bellman equation은 아래와 같이 표현된다. 여기서 Z(x, a)와 R(x, a)는 값이 아니라 분포다.

$Z(x,a) \overset {D}{=} R(x,a)+\gamma Z(x',a')$

또한 분포에 대한 계산이 가능한 loss 함수가 필요하다. 저자는 Wasserstein metric을 사용해 loss를 계산하는 방식을 먼저 사용했지만 한계가 있어, 결국 Kullback-Leibler (KL) divergence를 채택했다. 

### KL divergence

KL divergence는 두 확률분포의 차이를 계산할 때 사용하는 함수다.

- 이산확률변수: $D_{KL}(P||Q)= \underset{i}\sum P(i) log { P(i) \over Q(i)}$
- 연속확률변수: $D_{KL}(P||Q)= \underset{i}\sum p(x) log { p(x) \over q(x)} dx$

### 구현

저자는 어떤 정해진 수의 range(atom)를 정하고, 각 range마다 하나의 value를 가지게 해 분포를 나타냈다. 논문에서 좋은 결과를 보인 atom의 수는 51이다.

```python
def distr_projection(next_distr, rewards, dones, gamma):
		# 분포를 rewards와 gamma를 이용해 업데이트하고, 다시 원래 분포의 형태로 projection하는 코드
		batch_size = len(rewards)
		proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)
		# Projection의 결과를 저장할 배열 선언
		delta_z = (Vmax - Vmin) / (N_ATOMS - 1)
		for atom in range(N_ATOMS):
				v = rewards + (Vmin + atom * delta_z) * gamma
				tz_j = np.minimum(Vmax, np.maximum(Vmin, v))
		# 모든 atom에 대해 value를 구하고 어떤 atom에 속하게 될지 계산한다 
		b_j = (tz_j - Vmin) / delta_z
		l = np.floor(b_j).astype(np.int64)
		u = np.ceil(b_j).astype(np.int64)
		eq_mask = u == l
		proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]
		ne_mask = u != l
		proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]
		proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]
		if dones.any():
				proj_distr[dones] = 0.0
				tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))
				b_j = (tz_j - Vmin) / delta_z
				l = np.floor(b_j).astype(np.int64)
				u = np.ceil(b_j).astype(np.int64)
				eq_mask = u == l
				eq_dones = dones.copy()
				eq_dones[dones] = eq_mask
				if eq_dones.any():
						proj_distr[eq_dones, l[eq_mask]] = 1.0
				ne_mask = u != l
				ne_dones = dones.copy()
				ne_dones[dones] = ne_mask
				if ne_dones.any():
					proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]
					proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]
				return proj_distr

```

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.04.27.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.04.27.png)

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.06.25.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.06.25.png)

위 코드를 사용해 좌상단 그래프에서 gamma=0.9, reward=2에 의해 projected된 distribution은 좌하단 그래프와 같다. 만약 done=True이면 분포는 완전히 무시되고, 우하단 그래프처럼 reward만 project된다.

```python
Vmax = 10
Vmin = -10
N_ATOMS = 51
DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)

class DistributionalDQN(nn.Module):
		def __init__(self, input_shape, n_actions):
				super(DistributionalDQN, self).__init__()
				self.conv = nn.Sequential(
						nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
						nn.ReLU(),
						nn.Conv2d(32, 64, kernel_size=4, stride=2),
						nn.ReLU(),
						nn.Conv2d(64, 64, kernel_size=3, stride=1),
						nn.ReLU()
				)
				conv_out_size = self._get_conv_out(input_shape)
				self.fc = nn.Sequential(
						nn.Linear(conv_out_size, 512),
						nn.ReLU(),
						nn.Linear(512, n_actions * N_ATOMS)
				)
				# Output의 size가 n_actions * N_ATOMS
				sups = torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z)
				self.register_buffer("supports", sups)
				self.softmax = nn.Softmax(dim=1)

		def forward(self, x):
				batch_size = x.size()[0]
				fx = x.float() / 256
				conv_out = self.conv(fx).view(batch_size, -1)
				fc_out = self.fc(conv_out)
				return fc_out.view(batch_size, -1, N_ATOMS)

		def both(self, x):
				cat_out = self(x)
				probs = self.apply_softmax(cat_out)
				weights = probs * self.supports
				res = weights.sum(dim=2)
				return cat_out, res

		def qvals(self, x):
				return self.both(x)[1]

		def apply_softmax(self, t):
				return self.softmax(t.view(-1, N_ATOMS)).view(t.size())
```

```python
def calc_loss(batch, net, tgt_net, gamma, device="cpu"):
		states, actions, rewards, dones, next_states = common.unpack_batch(batch)
		batch_size = len(batch)
		states_v = torch.tensor(states).to(device)
		actions_v = torch.tensor(actions).to(device)
		next_states_v = torch.tensor(next_states).to(device)

		next_distr_v, next_qvals_v = tgt_net.both(next_states_v)
		next_acts = next_qvals_v.max(1)[1].data.cpu().numpy()
		next_distr = tgt_net.apply_softmax(next_distr_v)
		next_distr = next_distr.data.cpu().numpy()
		next_best_distr = next_distr[range(batch_size), next_acts]
		dones = dones.astype(np.bool)

		proj_distr = dqn_extra.distr_projection(
		   next_best_distr, rewards, dones, gamma)

		distr_v = net(states_v)
		sa_vals = distr_v[range(batch_size), actions_v.data]
		state_log_sm_v = F.log_softmax(sa_vals, dim=1)
		proj_distr_v = torch.tensor(proj_distr).to(device)

		loss_v = -state_log_sm_v * proj_distr_v
		return loss_v.sum(dim=1).mean()
```

### 결과

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.13.08.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__4.13.08.png)

Categorical DQN은 수렴하기까지 시간이 더 걸렸고 기존의 DQN보다 학습이 불안정했다. Atom이 51개인만큼 파라미터가 늘어났기 때문이라고 생각할 수 있다. Categorical DQN은 Pong에서는 좋은 결과를 보이지 않았지만, Pong 외의 다른 복잡한 Atari 환경에선 기존의 DQN보다 훨씬 좋은 성능을 보였다.

## Rainbow

위에서 언급한 6가지의 개선안의 효과를 확인하고 종합한 것이 Rainbow 논문이다. 이 때 categorical DQN과 double DQN은 Atari 기니피그 환경에서 큰 개선 효과를 보이지 못해서, 최종 모델에서 제외되었다.

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.49.59.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.49.59.png)

Reward dynamics of the combined system

![Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.50.47.png](Ch%208%20DQN%20Extensions%20016b834a51c54bc8b90b853a8b764095/_2021-07-01__12.50.47.png)

The number of steps in episodes

최종 Rainbow모델은 기존의 DQN에 비해 훨씬 높은 reward dynamics를 보여줬고, 더 빠르게 winning policy를 찾는 것을 알 수 있다.

## 의논해볼 것

- Double DQN의 Bellman 식의 수학적인 해석이 어렵다. 왜 max도 있고 argmax도 있는지?
- Reward dynamics의 해석이 어렵다. 책에서는 the average reward for episodes starts to grow earlier이라고 나온다.
- Dueling DQN에서 왜 A(s,a)가 시간이 지날수록 더 발전되었다고 하는 걸까?