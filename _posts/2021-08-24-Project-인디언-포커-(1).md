# 인디언 포커
방송 <더지니어스>에 등장한 인디언 포커는 우리가 알고 있는 인디언 포커(Blind man's bluff)의 룰을 일대일 상황에 맞춰 변형시킨 게임이다. 방송 클립을 보고, 주어지는 정보가 적은만큼 심리전이 중요한 게임이라는 생각이 들었다. 예를 들어 호승심이 높은 사람은 낮은 사람보다 베팅하는 칩의 수가 더 많고 상대가 더 높은 숫자의 카드를 가지고 있어도 드롭하지 않을 것이다. 
이에 에이전트에 호승심과 관련된 변수 또는 네트워크를 도입하거나, 상대의 전략 또는 심리를 예측해 유연하게 반응해 다양한 전략을 구사하는 상대에게 강인한 에이전트를 고안하려 한다. 이후 일반적인 방법으로 학습한 DQN, A3C 등의 에이전트와 인간 플레이어와의 게임 승률을 비교해보려 한다.

## 규칙

1. 두 플레이어는 30개씩 칩을 받는다.
2. 두 플레이어는 1부터 10까지의 카드 세트 2개에서 카드를 한 개씩 받고, 상대방에게만 보이게 한다.
3. 게임 시작과 함께 두 플레이어는 각각 1개의 칩을 배팅한다.
4. 선 플레이어부터 베팅을 진행한다. 베팅은 3가지 종류가 가능한데, 아래와 같다.
    1. 레이즈: 상대가 베팅한 칩보다 더 많은 칩을 베팅한다. 
    2. 오픈: 상대가 베팅한 칩만큼 베팅한다. 이 경우 즉시 두 플레이어의 패를 열고, 더 숫자가 큰 쪽이 베팅된 모든 칩을 가져간다.
    3. 드롭: 게임을 포기한다. 이 경우 상대가 베팅된 모든 칩을 가져가고 게임이 끝난다.
5. 게임이 끝날 때마다 얻거나 잃은 칩 만큼 리워드를 부여한다.
6. 한 플레이어가 모든 칩을 잃으면 에피소드가 종료된다.

[https://www.youtube.com/watch?v=MmHnOW6r_KU](https://www.youtube.com/watch?v=MmHnOW6r_KU)

## Agent
게임에 필요한 에이전트를 몇 가지 떠올려보았다.
- __init__(self)
- choose_action(self, x):
    에이전트의 Action space은 주어진 칩 수이다. 

### Random Agent
랜덤하게 행동하는 에이전트이다.
Inherits the class Agent

### Rule-based Agent
상대의 카드에만 의존해 행동하는 에이전트이다. 상대의 카드가 A 이상이면 드롭하고, A 이하이면 레이즈 또는 오픈한다. 이 때 레이즈할 칩 수(0이면 오픈)는 B에 따라 결정된다. A, B는 여러 값으로 바꾸어 가며 학습해 우리의 에이전트가 다양한 전략에 반응할 수 있도록 할 것이다.
Inherits the class Agent

## Environment

- __init__(self)
- observe(self): return card of opponent, betted chips of players, remaining chips of players, amount of chips opponent betted
- step(self)
    - raise: change the amount of betted chips
    - open: change the amount of betted chips, open cards, give rewards to agent
    - drop:  change the amount of betted chips, give rewards to agent
    - lose: give all remaining chips to opponent, give rewards, start new episode
    - return: observation, end of the episode (boolean), end of the game (boolean), reward

## Episode Flow

1. 에이전트: 초기화
2. 환경: 초기화
3. 환경: 선후 결정 
4. 선 에이전트: 1개 베팅
5. 후 에이전트: 1개 베팅
6. 선 에이전트: 관찰
7. 선 에이전트: 베팅 
8. 환경: 판정
9. 후 에이전트: 관찰
10. 후 에이전트: 베팅
11. 환경: 판정
12. 판정 결과 게임이 끝날 때까지 5~10 반복 
13. 잃거나 얻은 칩 수 만큼 보상
14. 한 에이전트의 칩이 0개가 될 때까지 2~12 반복

## 코드
환경과 학습 코드, 그리고 Random, Rule-based, DQN 에이전트를 구현했다. 
[https://colab.research.google.com/drive/1aB7dMcyOKjwSCUaRPTNU2rl0ysXkC96C?usp=sharing](https://colab.research.google.com/drive/1aB7dMcyOKjwSCUaRPTNU2rl0ysXkC96C?usp=sharing)

## 계획
1. Imperfect-information games 관련 논문 리뷰
2. 상대의 전략을 예측하는 네트워크를 가진 에이전트를 고안
3. 호승심과 관련된 파라미터에 따라 Action을 변형하는 네트워크를 가진 에이전트를 고안하고 Self-play
