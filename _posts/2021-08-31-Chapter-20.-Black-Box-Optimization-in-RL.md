### Black-box methods

- Not assuming the differentiability, value function, the smoothness of the objective, and so on
- Calculate the fitness function: measure of suitability of a particular instance of the optimized entity
- Random search: randomly sample the thing we're looking for → check the fitness → if it's good, take it

### Advantages of black-box methods

- Faster; no backpropagation step
- Don't expect much from the black-box internals → Can deal with non-smooth reward function, policy with randomness which traditional methods struggle with
- Parallelized very well

### Evolution strategies

- Covariance matrix adaptation evolution strategy (CMA-ES)
- Random perturbation (small change) of current policy → Evaluate the fitness → Adjust the policy weights proportionally to the fitness
- Perturbation: random noise sampled from the normal distribution with the zero mean and identity variance
- Adjust the original policy weights by adding the noise multiplied by the fitness function value → Higher fitness
- Update of the weight is performed by averaging the batch → Improve stability

### CMA-ES

![스크린샷 2021-09-05 오후 12.48.44.png](https://github.com/RL-Study-On/contrast/blob/master/assets/08-31-book-minji/3.png?raw=true)

### ES on Cartpole

[https://colab.research.google.com/drive/1IqeV2oLseD1-DV9sEHUiWnTzZ6CRgtEH?usp=sharing](https://colab.research.google.com/drive/1IqeV2oLseD1-DV9sEHUiWnTzZ6CRgtEH?usp=sharing)

### Genetic algorithms

- Generating a population of N individuals: some combination of model parameters → Evaluate with the fitness function → Some subset of top performers is used to produce (called mutation) the next generation of the population
- Variety of methods: way to mutate, way to rank the performers

→ Simple GA method

### GA with Gaussian noise mutation

![스크린샷 2021-09-05 오후 2.28.40.png](https://github.com/RL-Study-On/contrast/blob/master/assets/08-31-book-minji/1.png?raw=true)

![스크린샷 2021-09-05 오후 2.28.52.png](https://github.com/RL-Study-On/contrast/blob/master/assets/08-31-book-minji/2.png?raw=true)

Use Gaussian noise perturbation of the parent's weights to perform mutation

### GA on Cartpole

[https://colab.research.google.com/drive/1JXdhaYXCgyI0hqhXZoI_ZR2azoqsNmEe?usp=sharing](https://colab.research.google.com/drive/1JXdhaYXCgyI0hqhXZoI_ZR2azoqsNmEe?usp=sharing)

### GA tweaks

- Deep GA
- Novelty search

### Deep GA

- Method for parallelization
- Policy parameters are represented as a list of random seeds used to create this particular policy's weights
- Initial network's weights were generated randomly on the first population → First seed in the list defines this initialization and mutations
- Reconstruct the weights on every worker: less time than transferring full weights over the network

### Novelty search

- Objective: Increase the total reward → reward the agent for exploring the behavior that is has never explored before, i.e., novel
