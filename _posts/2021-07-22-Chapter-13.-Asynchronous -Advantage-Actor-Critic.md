### Correlation

Policy gradient method의 안정성을 증대하기 위해서는 여러 환경을 parallel하게 사용하는 것이 좋다는 것이 알려져 있다. 이는 sample 간의 correlation 때문에 independent and identically distributed (i.i.d.)의 가정을 위배하게 되어 SGD에 방해가 되기 때문이다. 강화학습에서 환경에서 수집한 sample은 supervised learning의 데이터이므로 서로 다른 라벨의 이미지의 수를 비슷하게 맞춰주는 이유랑 비슷한 것 같다. 

DQN에서는 experience replay를 통해 sample 간의 correlation을 낮췄지만 대부분이 on-policy, 즉, 과거의 policy로 수집한 데이터를 사용하지 않는 policy gradient method에선 그 방법을 쓰기 어렵다. 그에 따라 똑같은 policy로 여러 환경을 탐색해 각 에피소드에서 수집된 데이터를 parallel하게 사용하는 방법, A3C의 A 중 하나를 차지하는 asynchronous한 방법이 제안되었다. 

### Sample inefficiency

하지만 policy gradient method은 sample efficieny가 떨어진다. Sample을 experience replay에 수집하고 계속 사용하는 DQN과 달리 policy gradient method은 sample을 특정 policy가 사용된 딱 한 순간밖에 사용하지 못한다.

### Asynchronous Actor-Critic

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/1.png?raw=true)

An agent training from multiple environments in parallel

사실 asynchronous한 방법은 떠올리기 어렵지 않다. 여러 환경에서 동시에 학습을 하고 transition을 모두 수집해서 학습에 사용하면 된다. 그러나 이는 엄밀히 말하면 parallel하다고 할 수 없다. 환경에서 action을 취하고, transition을 수집하고 학습하는 것 모두 serial하기 때문이다. 

### Actor-critic parallelization

Pytorch에 내장된 torch.multiprocessing 모듈 덕분에 parallel computing을 이용하기는 어렵지 않다. A3C에서 주요하게 parallelization이 필요한 것은 data와 gradient이다.

- Data parallelism: 수집된 transition을 통해서 NN을 학습시켜야하는데, transition을 수집하기 위해선 NN에 state를 입력해야한다. 한 환경에서 수집된 transition으로 update를 한 NN으로 다음 환경의 transition을 수집한다면 이는 parallel하지 않다. 그러므로 모든 transition은 동일한 NN으로 수집되어야한다.
- Gradients parallelism: 서로 다른 환경의 sample들에서 각각 계산된 gradient를 합쳐서 한 번의 update를 거쳐야 한다.

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/2.png?raw=true)
![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/3?raw=true)

### Data parallelism code

```python
PROCESSES_COUNT = 4
NUM_ENVS = 8
MICRO_BATCH_SIZE = 32
```

```python
def make_env():
		return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME))

TotalReward = collections.namedtuple('TotalReward', field_names='reward')

def data_func(net, device, train_queue):
    envs = [make_env() for _ in range(NUM_ENVS)]
    agent = ptan.agent.PolicyAgent(
        lambda x: net(x)[0], device=device, apply_softmax=True)
    exp_source = ptan.experience.ExperienceSourceFirstLast(
        envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)
    micro_batch = []

    for exp in exp_source:
        new_rewards = exp_source.pop_total_rewards()
        if new_rewards:
            data = TotalReward(reward=np.mean(new_rewards))
            train_queue.put(data)

        micro_batch.append(exp)
        if len(micro_batch) < MICRO_BATCH_SIZE:
            continue

        data = common.unpack_batch(
            micro_batch, net, device=device,
            last_val_gamma=GAMMA ** REWARD_STEPS)
        train_queue.put(data)
        micro_batch.clear()
```

**main 함수**

```python
env = make_env()
net = common.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)
net.share_memory()

optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)

train_queue = mp.Queue(maxsize=PROCESSES_COUNT)
# 복사 없이 child process의 데이터를 공유할 수 있는 큐
# maxsize=PROCESSES_COUNT여서 현재 policy의 데이터만 쓰게 됨

data_proc_list = []
for _ in range(PROCESSES_COUNT):
    data_proc = mp.Process(target=data_func, args=(net, device, train_queue))
		# target은 함수, args는 해당 함수의 argument
    data_proc.start()
    data_proc_list.append(data_proc)

batch_states = []
batch_actions = []
batch_vals_ref = []
step_idx = 0
batch_size = 0
```

```python
# Tensorboard 를 제외한 코드
try:
    while True:
        train_entry = train_queue.get()
# Train queue 에서 entry를 하나 가져옴

        states_t, actions_t, vals_ref_t = train_entry
        batch_states.append(states_t)
        batch_actions.append(actions_t)
        batch_vals_ref.append(vals_ref_t)
        step_idx += states_t.size()[0]
        batch_size += states_t.size()[0]
        if batch_size < BATCH_SIZE:
            continue
# BATCH_SIZE만큼 데이터를 사용

        states_v = torch.cat(batch_states)
        actions_t = torch.cat(batch_actions)
        vals_ref_v = torch.cat(batch_vals_ref)
        batch_states.clear()
        batch_actions.clear()
        batch_vals_ref.clear()
        batch_size = 0

        optimizer.zero_grad()
        logits_v, value_v = net(states_v)

        loss_value_v = F.mse_loss(
            value_v.squeeze(-1), vals_ref_v)

        log_prob_v = F.log_softmax(logits_v, dim=1)
        adv_v = vals_ref_v - value_v.detach()
        size = states_v.size()[0]
        log_p_a = log_prob_v[range(size), actions_t]
        log_prob_actions_v = adv_v * log_p_a
        loss_policy_v = -log_prob_actions_v.mean()

        prob_v = F.softmax(logits_v, dim=1)
        ent = (prob_v * log_prob_v).sum(dim=1).mean()
        entropy_loss_v = ENTROPY_BETA * ent

        loss_v = entropy_loss_v + loss_value_v + \
                 loss_policy_v
        loss_v.backward()
        nn_utils.clip_grad_norm_(
            net.parameters(), CLIP_GRAD)
        optimizer.step()

finally:
# Game solved or terminated
    for p in data_proc_list:
        p.terminate()
        p.join()
# Terminate all child processes
```

### Gradient parallelism code

Gradient를 계산해야하기 때문에 grad_func가 data_func과 많이 달라진다.

```python
def grads_func(proc_name, net, device, train_queue):
    envs = [make_env() for _ in range(NUM_ENVS)]

    agent = ptan.agent.PolicyAgent(
        lambda x: net(x)[0], device=device, apply_softmax=True)
    exp_source = ptan.experience.ExperienceSourceFirstLast(
        envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    batch = []
    frame_idx = 0
		
		for exp in exp_source:
        frame_idx += 1
        new_rewards = exp_source.pop_total_rewards()
        if new_rewards and tracker.reward(
                new_rewards[0], frame_idx):
            break

        batch.append(exp)
        if len(batch) < GRAD_BATCH:
            continue

        data = common.unpack_batch(
            batch, net, device=device,
            last_val_gamma=GAMMA**REWARD_STEPS)
        states_v, actions_t, vals_ref_v = data

        batch.clear()

        net.zero_grad()
        logits_v, value_v = net(states_v)
        loss_value_v = F.mse_loss(
            value_v.squeeze(-1), vals_ref_v)

        log_prob_v = F.log_softmax(logits_v, dim=1)
        adv_v = vals_ref_v - value_v.detach()
        log_p_a = log_prob_v[range(GRAD_BATCH), actions_t]
        log_prob_actions_v = adv_v * log_p_a
        loss_policy_v = -log_prob_actions_v.mean()

        prob_v = F.softmax(logits_v, dim=1)
        ent = (prob_v * log_prob_v).sum(dim=1).mean()
        entropy_loss_v = ENTROPY_BETA * ent

        loss_v = entropy_loss_v + loss_value_v + \
                 loss_policy_v
        loss_v.backward()

        # gather gradients
        nn_utils.clip_grad_norm_(
            net.parameters(), CLIP_GRAD)
        grads = [
            param.grad.data.cpu().numpy()
            if param.grad is not None else None
            for param in net.parameters()
        ]
        train_queue.put(grads)

    train_queue.put(None)
```

상대적으로 main 코드는 단순해진다.

```python
try:
    while True:
        train_entry = train_queue.get()
        if train_entry is None:
            break

        step_idx += 1

        if grad_buffer is None:
            grad_buffer = train_entry
        else:
            for tgt_grad, grad in zip(grad_buffer, train_entry):
                tgt_grad += grad

        if step_idx % TRAIN_BATCH == 0:
# Gradients가 TRAIN_BATCH의 수만큼 모인 경우
            **for param, grad in zip(net.parameters(), grad_buffer):
                param.grad = torch.FloatTensor(grad).to(device)

            nn_utils.clip_grad_norm_(
                net.parameters(), CLIP_GRAD)**
# 의 L2 norm이 CLIP_GRAD(=0.1) 보다 작도록 clipping
            **optimizer.step()
            grad_buffer = None**
```

**Gradient clipping**

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/6.png?raw=true)

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/7.png?raw=true)

### Result

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/4.png?raw=true)

The reward (left) and total loss (right) of the data-parallel version

![image title](https://github.com/RL-Study-On/contrast/blob/master/assets/07-22-book-minji/5.png?raw=true)

The mean reward (left) and total loss (right) for the gradient-parallel version
